import json
import os
import sys
import time
from typing import List, Tuple
from pathlib import Path

import numpy as np
import openai
from openai import OpenAI
import faiss
from sqlalchemy import text
from sqlalchemy.orm import Session

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.database import SessionLocal

# é…ç½®å‚æ•°
EMBED_MODEL = "text-embedding-3-small"
INDEX_PATH = os.getenv("VECTOR_INDEX_PATH", "backend/vector_index/faiss.index")
ID_MAP_PATH = os.getenv("VECTOR_ID_MAP_PATH", "backend/vector_index/id_map.json")
BATCH_SIZE = 100  # OpenAI APIæ‰¹é‡å¤„ç†é™åˆ¶
MAX_RETRIES = 3  # é‡è¯•æ¬¡æ•°


def get_all_pose_text(db: Session) -> List[Tuple[int, str]]:
    """è·å–æ‰€æœ‰å§¿åŠ¿çš„æ–‡æœ¬æ•°æ®"""
    sql = text(
        """
        SELECT p.id, p.title, p.description, p.scene_category, 
               p.angle, p.shooting_tips, p.ai_tags,
               GROUP_CONCAT(t.name SEPARATOR ' ') as tags
        FROM poses p
        LEFT JOIN pose_tags pt ON p.id = pt.pose_id
        LEFT JOIN tags t ON pt.tag_id = t.id
        WHERE p.status = 'active'
        GROUP BY p.id, p.title, p.description, p.scene_category, 
                 p.angle, p.shooting_tips, p.ai_tags
        """
    )
    
    try:
        rows = db.execute(sql).fetchall()
        results = []
        
        for row in rows:
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å­—æ®µ
            text_parts = []
            for field in row[1:]:  # è·³è¿‡IDå­—æ®µ
                if field and str(field).strip():
                    text_parts.append(str(field).strip())
            
            combined_text = " ".join(text_parts)
            if combined_text.strip():  # åªä¿ç•™æœ‰å†…å®¹çš„è®°å½•
                results.append((row[0], combined_text.strip()))
        
        print(f"æ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆå§¿åŠ¿è®°å½•")
        return results
        
    except Exception as e:
        print(f"è·å–å§¿åŠ¿æ•°æ®å¤±è´¥: {e}")
        return []


def embed_text_batch(texts: List[str], client: OpenAI) -> List[List[float]]:
    """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    for attempt in range(MAX_RETRIES):
        try:
            print(f"æ­£åœ¨ç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡...")
            resp = client.embeddings.create(input=texts, model=EMBED_MODEL)
            embeddings = [d.embedding for d in resp.data]
            print(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
            return embeddings
            
        except Exception as e:
            print(f"åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            else:
                raise e


def build_index():
    """æ„å»ºå‘é‡ç´¢å¼•"""
    print("å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
    
    # æ£€æŸ¥OpenAI APIå¯†é’¥
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return False
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    try:
        client = OpenAI(api_key=api_key)
        print("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # è·å–æ•°æ®åº“ä¼šè¯
    db = SessionLocal()
    try:
        # è·å–æ‰€æœ‰å§¿åŠ¿æ–‡æœ¬æ•°æ®
        data = get_all_pose_text(db)
        if not data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å§¿åŠ¿æ•°æ®")
            return False
        
        ids, texts = zip(*data)
        print(f"å‡†å¤‡å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬è®°å½•")
        
        # åˆ†æ‰¹å¤„ç†æ–‡æœ¬åµŒå…¥
        all_embeddings = []
        total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE
        
        for i in range(0, len(texts), BATCH_SIZE):
            batch_texts = texts[i:i + BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            
            print(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_texts)} ä¸ªæ–‡æœ¬)")
            
            try:
                batch_embeddings = embed_text_batch(list(batch_texts), client)
                all_embeddings.extend(batch_embeddings)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
                if i + BATCH_SIZE < len(texts):
                    time.sleep(1)
                    
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                return False
        
        if len(all_embeddings) != len(texts):
            print(f"âŒ åµŒå…¥å‘é‡æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(texts)}, å®é™… {len(all_embeddings)}")
            return False
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(all_embeddings)} ä¸ªåµŒå…¥å‘é‡")
        
        # æ„å»ºFAISSç´¢å¼•
        print("æ„å»ºFAISSç´¢å¼•...")
        embeddings_array = np.array(all_embeddings, dtype="float32")
        dimension = embeddings_array.shape[1]
        
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings_array)
        
        print(f"FAISSç´¢å¼•åˆ›å»ºå®Œæˆï¼Œç»´åº¦: {dimension}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(index, INDEX_PATH)
        print(f"âœ… FAISSç´¢å¼•å·²ä¿å­˜åˆ°: {INDEX_PATH}")
        
        # ä¿å­˜IDæ˜ å°„ï¼ˆä½¿ç”¨å­—ç¬¦ä¸²é”®ä»¥åŒ¹é…vector_search_service.pyï¼‰
        id_map = {str(i): pose_id for i, pose_id in enumerate(ids)}
        with open(ID_MAP_PATH, "w", encoding="utf-8") as f:
            json.dump(id_map, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… IDæ˜ å°„å·²ä¿å­˜åˆ°: {ID_MAP_PATH}")
        print(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ! åŒ…å« {len(ids)} æ¡è®°å½•")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ„å»ºç´¢å¼•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        db.close()


def verify_index():
    """éªŒè¯æ„å»ºçš„ç´¢å¼•"""
    print("\néªŒè¯ç´¢å¼•æ–‡ä»¶...")
    
    try:
        if not os.path.exists(INDEX_PATH):
            print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {INDEX_PATH}")
            return False
            
        if not os.path.exists(ID_MAP_PATH):
            print(f"âŒ IDæ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {ID_MAP_PATH}")
            return False
        
        # éªŒè¯FAISSç´¢å¼•
        index = faiss.read_index(INDEX_PATH)
        print(f"âœ… FAISSç´¢å¼•: {index.ntotal} ä¸ªå‘é‡, ç»´åº¦ {index.d}")
        
        # éªŒè¯IDæ˜ å°„
        with open(ID_MAP_PATH, "r", encoding="utf-8") as f:
            id_map = json.load(f)
        print(f"âœ… IDæ˜ å°„: {len(id_map)} ä¸ªæ¡ç›®")
        
        if index.ntotal != len(id_map):
            print(f"âš ï¸  è­¦å‘Š: ç´¢å¼•å‘é‡æ•°é‡ ({index.ntotal}) ä¸IDæ˜ å°„æ•°é‡ ({len(id_map)}) ä¸åŒ¹é…")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç´¢å¼•éªŒè¯å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=== å‘é‡ç´¢å¼•æ„å»ºå·¥å…· ===")
    print(f"åµŒå…¥æ¨¡å‹: {EMBED_MODEL}")
    print(f"ç´¢å¼•è·¯å¾„: {INDEX_PATH}")
    print(f"æ˜ å°„è·¯å¾„: {ID_MAP_PATH}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print()
    
    # æ„å»ºç´¢å¼•
    success = build_index()
    
    if success:
        # éªŒè¯ç´¢å¼•
        verify_success = verify_index()
        if verify_success:
            print("\nğŸ‰ å‘é‡ç´¢å¼•æ„å»ºå’ŒéªŒè¯å®Œæˆ!")
            return 0
        else:
            print("\nâŒ ç´¢å¼•éªŒè¯å¤±è´¥")
            return 1
    else:
        print("\nâŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)